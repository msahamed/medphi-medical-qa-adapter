{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f1f876-00c6-40b1-9979-ad0814079d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_from_disk\n",
    "from transformers import pipeline, StoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a35b0566-f558-437c-9590-d93696d3d982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f8a15a31eb4c25834093452deab97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./lora_adapter\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "178d3b78-11ae-40b5-b196-705b6867d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"./dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24ee729b-d632-4eda-9e29-edd2c0b34e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 12304\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d75bc42-fcae-4d81-804c-e2ffa9457ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    # https://huggingface.co/microsoft/MediPhi-Instruct\n",
    "    system_message = \"\"\"\n",
    "    You are a smart medical assiatnt to help user question about their queries\n",
    "    \n",
    "    To answer question, follow the following instructions:\n",
    "    1. **Understand the question**: Clearly identify the question and any important given values.\n",
    "    3. **Answer Step-by-Step**: Iteratively progress your answer\n",
    "    4. **Double Check**: If applicable, double check the question for accuracy and sense.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hugging Face pipeline for text generation does apply apply_chat_template under the hood. \n",
    "    # So we do not need to process for the text generation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=ft_model,\n",
    "        tokenizer=ft_tokenizer,\n",
    "    )\n",
    "    \n",
    "    #  stops generation when the model generates token ID 32007\n",
    "    class EosListStoppingCriteria(StoppingCriteria):\n",
    "      def __init__(self, eos_sequence = [32007]):\n",
    "          self.eos_sequence = eos_sequence\n",
    "    \n",
    "      def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "          last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "          return self.eos_sequence in last_ids\n",
    "    \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "        \"stopping_criteria\": [EosListStoppingCriteria()]\n",
    "    \n",
    "    }\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "313a4456-d247-4e5e-9ffc-6228509d32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_test_result(iterations=10):\n",
    "    for i in range(iterations):\n",
    "        index = np.random.randint(0, len(dataset['test']))\n",
    "        question = dataset['test'][index]['question']  # Fixed: was 'answer'\n",
    "        answer = dataset['test'][index]['answer']\n",
    "        \n",
    "        output = generate_answer(question)\n",
    "        \n",
    "        # Header with test number\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"🧪 TEST {i+1}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Question section\n",
    "        print(\"\\n🤔 QUESTION:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{question}\")\n",
    "        \n",
    "        # Original answer section\n",
    "        print(\"\\n✅ ORIGINAL ANSWER:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{answer}\")\n",
    "        \n",
    "        # AI response section\n",
    "        print(\"\\n🤖 FINE-TUNED AI RESPONSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"{output[0]['generated_text']}\")\n",
    "        \n",
    "        # Separator\n",
    "        print(\"\\n\" + \"~\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca79165a-1108-45c6-8782-c8d2cdbaac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 1\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "How many people are affected by Lenz microphthalmia syndrome ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "Lenz microphthalmia syndrome is a very rare condition; its incidence is unknown. It has been identified in only a few families worldwide.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " To answer this question, I would need to conduct research as the number of people affected by Lenz microphthalmia syndrome is not readily available in the current database.\n",
      "\n",
      "1. **Research**: I would start by searching for recent studies or databases that track genetic disorders.\n",
      "2. **Data Collection**: I would look for information on the prevalence of Lenz microphthalmia syndrome.\n",
      "3. **Analysis**: I would analyze the data to provide an estimate of the number of people affected.\n",
      "4. **Answer**: Based on the research, I would provide an answer, such as \"Lenz microphthalmia syndrome is a rare genetic disorder, and the exact number of affected individuals is not well-documented. However, it is estimated to affect approximately 1 in 100,000 people.\"\n",
      "\n",
      "Please note that the actual number may vary based on new research or data.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 2\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What are the symptoms of Horizontal gaze palsy with progressive scoliosis ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "What are the signs and symptoms of Horizontal gaze palsy with progressive scoliosis? The Human Phenotype Ontology provides the following list of signs and symptoms for Horizontal gaze palsy with progressive scoliosis. If the information is available, the table below includes how often the symptom is seen in people with this condition. You can use the MedlinePlus Medical Dictionary to look up the definitions for these medical terms. Signs and Symptoms Approximate number of patients (when available) Kyphosis 90% Cognitive impairment 50% Nystagmus 50% Short neck 50% Seizures 7.5% Sensorineural hearing impairment 7.5% Autosomal recessive inheritance - Congenital onset - Horizontal supranuclear gaze palsy - Progressive ophthalmoplegia - Thoracolumbar scoliosis - The Human Phenotype Ontology (HPO) has collected information on how often a sign or symptom occurs in a condition. Much of this information comes from Orphanet, a European rare disease database. The frequency of a sign or symptom is usually listed as a rough estimate of the percentage of patients who have that feature. The frequency may also be listed as a fraction. The first number of the fraction is how many people had the symptom, and the second number is the total number of people who were examined in one study. For example, a frequency of 25/25 means that in a study of 25 people all patients were found to have that symptom. Because these frequencies are based on a specific study, the fractions may be different if another group of patients are examined. Sometimes, no information on frequency is available. In these cases, the sign or symptom may be rare or common.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " To answer this question, I will follow these steps:\n",
      "\n",
      "1. **Understand the question**: The user is asking about the symptoms of a specific medical condition, Horizontal gaze palsy with progressive scoliosis.\n",
      "\n",
      "2. **Research**: I will look up the symptoms associated with this condition.\n",
      "\n",
      "3. **Answer Step-by-Step**:\n",
      "   - **Symptom 1**: Difficulty in moving the eyes horizontally.\n",
      "   - **Symptom 2**: Progressive curvature of the spine, known as scoliosis.\n",
      "   - **Symptom 3**: Possible head tilt or turn to compensate for gaze palsy.\n",
      "   - **Symptom 4**: Difficulty with balance and coordination.\n",
      "   - **Symptom 5**: Potential vision problems due to gaze palsy.\n",
      "\n",
      "4. **Double Check**: Ensure that the symptoms listed are accurate and relevant to the condition.\n",
      "\n",
      "The symptoms of Horizontal gaze palsy with progressive scoliosis include difficulty in moving the eyes horizontally, progressive curvature of the spine, possible head tilt or turn to compensate for gaze palsy, difficulty with balance and coordination, and potential vision problems due to gaze palsy.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 3\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What is (are) Apraxia ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "Apraxia (called \"dyspraxia\" if mild) is a neurological disorder characterized by loss of the ability to execute or carry out skilled movements and gestures, despite having the desire and the physical ability to perform them. Apraxia results from dysfunction of the cerebral hemispheres of the brain, especially the parietal lobe, and can arise from many diseases or damage to the brain.  There are several kinds of apraxia, which may occur alone or together. The most common is buccofacial or orofacial apraxia, which causes the inability to carry out facial movements on command such as licking lips, whistling, coughing, or winking. Other types of apraxia include limb-kinetic apraxia (the inability to make fine, precise movements with an arm or leg), ideomotor apraxia (the inability to make the proper movement in response to a verbal command), ideational apraxia (the inability to coordinate activities with multiple, sequential movements, such as dressing, eating, and bathing), verbal apraxia (difficulty coordinating mouth and speech movements), constructional apraxia (the inability to copy, draw, or construct simple figures), and oculomotor apraxia (difficulty moving the eyes on command). Apraxia may be accompanied by a language disorder called aphasia. Corticobasal ganglionic degeneration is a disease that causes a variety of types of apraxia, especially in elderly adults.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " Apraxia is a neurological disorder characterized by the inability to perform purposeful movements or gestures, despite having the physical ability and desire to do so. It is often caused by damage to the brain, particularly in areas responsible for motor planning and execution. There are several types of apraxia, including ideomotor apraxia, ideational apraxia, and constructional apraxia, each affecting different aspects of movement and coordination. Treatment typically involves occupational therapy and rehabilitation to help individuals regain as much function as possible.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 4\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "How to diagnose Urinary Incontinence in Men ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "Men should tell a health care professional, such as a family practice physician, a nurse, an internist, or a urologista doctor who specializes in urinary problemsthey have UI, even if they feel embarrassed. To diagnose UI, the health care professional will\n",
      "                \n",
      "- take a medical history  - conduct a physical exam  - order diagnostic tests\n",
      "                \n",
      "Medical History\n",
      "                \n",
      "Taking a medical history can help a health care professional diagnose UI. He or she will ask the patient or caretaker to provide a medical history, a review of symptoms, a description of eating habits, and a list of prescription and over-the-counter medications the patient is taking. The health care professional will ask about current and past medical conditions.\n",
      "                \n",
      "The health care professional also will ask about the mans pattern of urination and urine leakage. To prepare for the visit with the health care professional, a man may want to keep a bladder diary for several days beforehand. Information that a man should record in a bladder diary includes\n",
      "                \n",
      "- the amount and type of liquid he drinks  - how many times he urinates each day and how much urine is released  - how often he has accidental leaks  - whether he felt a strong urge to go before leaking  - what he was doing when the leak occurred, for example, coughing or lifting  - how long the symptoms have been occurring\n",
      "                \n",
      "Use the Daily Bladder Diary to prepare for the appointment.\n",
      "                \n",
      "The health care professional also may ask about other lower urinary tract symptoms that may indicate a prostate problem, such as\n",
      "                \n",
      "- problems starting a urine stream  - problems emptying the bladder completely  - spraying urine  - dribbling urine  - weak stream  - recurrent UTIs  - painful urination\n",
      "                \n",
      "Physical Exam\n",
      "                \n",
      "A physical exam may help diagnose UI. The health care professional will perform a physical exam to look for signs of medical conditions that may cause UI. The health care professional may order further neurologic testing if necessary.\n",
      "                \n",
      "Digital rectal exam. The health care professional also may perform a digital rectal exam. A digital rectal exam is a physical exam of the prostate and rectum. To perform the exam, the health care professional has the man bend over a table or lie on his side while holding his knees close to his chest. The health care professional slides a gloved, lubricated finger into the patients rectum and feels the part of the prostate that lies in front of the rectum. The digital rectal exam is used to check for stool or masses in the rectum and to assess whether the prostate is enlarged or tender, or has other abnormalities. The health care professional may perform a prostate massage during a digital rectal exam to collect a sample of prostate fluid that he or she can test for signs of infection.\n",
      "                \n",
      "The health care professional may diagnose the type of UI based on the medical history and physical exam, or he or she may use the findings to determine if a man needs further diagnostic testing.\n",
      "                \n",
      "Diagnostic Tests\n",
      "                \n",
      "The health care professional may order one or more of the following diagnostic tests based on the results of the medical history and physical exam:\n",
      "                \n",
      "- Urinalysis. Urinalysis involves testing a urine sample. The patient collects a urine sample in a special container at home, at a health care professionals office, or at a commercial facility. A health care professional tests the sample during an office visit or sends it to a lab for analysis. For the test, a nurse or technician places a strip of chemically treated paper, called a dipstick, into the urine. Patches on the dipstick change color to indicate signs of infection in urine.  - Urine culture. A health care professional performs a urine culture by placing part of a urine sample in a tube or dish with a substance that encourages any bacteria present to grow. A man collects the urine sample in a special container in a health care professionals office or a commercial facility. The office or facility tests the sample onsite or sends it to a lab for culture. A health care professional can identify bacteria that multiply, usually in 1 to 3 days. A health care professional performs a urine culture to determine the best treatment when urinalysis indicates the man has a UTI. More information is provided in the NIDDK health topic, Urinary Tract Infections in Adults.  - Blood test. A blood test involves drawing blood at a health care professionals office or a commercial facility and sending the sample to a lab for analysis. The blood test can show kidney function problems or a chemical imbalance in the body. The lab also will test the blood to assess the level of prostate-specific antigen, a protein produced by prostate cells that may be higher in men with prostate cancer.  - Urodynamic testing. Urodynamic testing includes a variety of procedures that look at how well the bladder and urethra store and release urine. A health care professional performs urodynamic tests during an office visit or in an outpatient center or a hospital. Some urodynamic tests do not require anesthesia; others may require local anesthesia. Most urodynamic tests focus on the bladders ability to hold urine and empty steadily and completely; they may include the following:      - uroflowmetry, which measures how rapidly the bladder releases urine    - postvoid residual measurement, which evaluates how much urine remains in the bladder after urination    - reduced urine flow or residual urine in the bladder, which often suggests urine blockage due to BPH\n",
      "                \n",
      "More information is provided in the NIDDK health topic, Urodynamic Testing.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " To diagnose urinary incontinence in men, follow these steps:\n",
      "\n",
      "1. **Patient History**: Begin by taking a detailed medical history, focusing on urinary symptoms such as frequency, urgency, nocturia, and episodes of incontinence.\n",
      "\n",
      "2. **Physical Examination**: Conduct a physical examination, including a digital rectal exam (DRE) to assess prostate size and any abnormalities.\n",
      "\n",
      "3. **Urinalysis**: Perform a urinalysis to check for infection, blood, or other abnormalities in the urine.\n",
      "\n",
      "4. **Urodynamic Testing**: Consider urodynamic tests to evaluate bladder function and urine flow.\n",
      "\n",
      "5. **Imaging**: Use imaging studies like ultrasound or CT scan if needed to rule out anatomical causes.\n",
      "\n",
      "6. **Referral to Specialist**: If necessary, refer the patient to a urologist for further evaluation and management.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 5\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What is (are) Genetic Disorders ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "Genes are the building blocks of heredity. They are passed from parent to child. They hold DNA, the instructions for making proteins. Proteins do most of the work in cells. They move molecules from one place to another, build structures, break down toxins, and do many other maintenance jobs.    Sometimes there is a mutation, a change in a gene or genes. The mutation changes the gene's instructions for making a protein, so the protein does not work properly or is missing entirely. This can cause a medical condition called a genetic disorder.    You can inherit a gene mutation from one or both parents. A mutation can also happen during your lifetime.    There are three types of genetic disorders:       - Single-gene disorders, where a mutation affects one gene. Sickle cell anemia is an example.    - Chromosomal disorders, where chromosomes (or parts of chromosomes) are missing or changed. Chromosomes are the structures that hold our genes. Down syndrome is a chromosomal disorder.    - Complex disorders, where there are mutations in two or more genes. Often your lifestyle and environment also play a role. Colon cancer is an example.       Genetic tests on blood and other tissue can identify genetic disorders.    NIH: National Library of Medicine\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " Genetic disorders are diseases that result from changes or mutations in an individual's DNA. These mutations can be inherited from one or both parents or can occur spontaneously. Genetic disorders can affect various parts of the body and can lead to a wide range of health issues.\n",
      "\n",
      "Here are some key points about genetic disorders:\n",
      "\n",
      "1. **Inheritance Patterns**: Genetic disorders can be inherited in several ways, including autosomal dominant, autosomal recessive, X-linked, or mitochondrial inheritance.\n",
      "\n",
      "2. **Examples**: Some well-known genetic disorders include cystic fibrosis, sickle cell anemia, Huntington's disease, and Down syndrome.\n",
      "\n",
      "3. **Diagnosis**: Genetic disorders are often diagnosed through genetic testing, which can identify specific mutations in an individual's DNA.\n",
      "\n",
      "4. **Management**: While some genetic disorders have no cure, management strategies can include medication, therapy, lifestyle changes, and in some cases, surgical interventions.\n",
      "\n",
      "5. **Prevention**: Genetic counseling can help individuals and families understand their risks and make informed decisions about family planning and preventive measures.\n",
      "\n",
      "In summary, genetic disorders are conditions caused by alterations in the genetic material, and they can have significant impacts on an individual's health and well-being.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 6\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "How many people are affected by 46,XX testicular disorder of sex development ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "Approximately 1 in 20,000 individuals with a male appearance have 46,XX testicular disorder.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " To answer this question, I will need to conduct research to find the most recent and reliable data on the prevalence of 46,XX testicular disorder of sex development.\n",
      "\n",
      "Step 1: Research\n",
      "I will search for scientific studies, medical databases, and health organization reports to gather information on the prevalence of 46,XX testicular disorder of sex development.\n",
      "\n",
      "Step 2: Data Analysis\n",
      "After collecting the data, I will analyze it to determine the number of people affected by this condition.\n",
      "\n",
      "Step 3: Answer Formulation\n",
      "Based on the data analysis, I will formulate a clear and concise answer to the question.\n",
      "\n",
      "Step 4: Verification\n",
      "I will verify the accuracy of the information by cross-referencing multiple sources.\n",
      "\n",
      "According to the available data, the prevalence of 46,XX testicular disorder of sex development is estimated to be approximately 1 in 20,000 to 1 in 50,000 live births. This means that for every 20,000 to 50,000 live births, there is an estimated 1 individual affected by this condition.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 7\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What is (are) deafness-dystonia-optic neuronopathy syndrome ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "Deafness-dystonia-optic neuronopathy (DDON) syndrome, also known as Mohr-Tranebjrg syndrome, is characterized by hearing loss that begins early in life, problems with movement, impaired vision, and behavior problems. This condition occurs almost exclusively in males.  The first symptom of DDON syndrome is hearing loss caused by nerve damage in the inner ear (sensorineural hearing loss), which begins in early childhood. The hearing impairment worsens over time, and most affected individuals have profound hearing loss by age 10.  People with DDON syndrome typically begin to develop problems with movement during their teens, although the onset of these symptoms varies among affected individuals. Some people experience involuntary tensing of the muscles (dystonia), while others have difficulty coordinating movements (ataxia). The problems with movement usually worsen over time.  Individuals with DDON syndrome have normal vision during childhood, but they may begin to develop an increased sensitivity to light (photophobia) or other vision problems during their teens. These people often have a slowly progressive reduction in the sharpness of vision (visual acuity) and become legally blind in mid-adulthood.  People with this condition may also have behavior problems, including changes in personality and aggressive or paranoid behaviors. They also usually develop a gradual decline in thinking and reasoning abilities (dementia) in their forties. The lifespan of individuals with DDON syndrome depends on the severity of the disorder. People with severe cases have survived into their teenage years, while those with milder cases have lived into their sixties.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " Deafness-dystonia-optic neuronopathy syndrome is a rare neurological disorder characterized by a combination of symptoms including deafness, dystonia (involuntary muscle contractions), and optic neuronopathy (damage to the optic nerve). This syndrome is often associated with mutations in the SLC25A19 gene, which is involved in mitochondrial function.\n",
      "\n",
      "The condition typically presents in childhood or early adulthood, with symptoms progressing over time. Management of the syndrome is challenging and often requires a multidisciplinary approach, including audiology, neurology, and ophthalmology. Treatment may involve medications to manage symptoms, such as muscle relaxants for dystonia, and supportive therapies like hearing aids or cochlear implants for deafness.\n",
      "\n",
      "It is important to note that this syndrome is rare, and research is ongoing to better understand its pathophysiology and develop more effective treatments.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 8\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What is (are) High Blood Cholesterol ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "If TLC (Therapeutic Lifestyle Changes) cannot lower your LDL cholesterol level enough by itself, your doctor may prescribe cholesterol-lowering medicines. The following medicines are used together with TLC to help lower your LDL (bad) cholesterol level. - statins  - ezetimibe  - bile acid sequestrants  - nicotinic acid  - fibrates. statins ezetimibe bile acid sequestrants nicotinic acid fibrates. Statins  - are very effective in lowering LDL (bad) cholesterol levels   - are safe for most people  - have side effects that are infrequent, but potentially serious such as liver and muscle problems. are very effective in lowering LDL (bad) cholesterol levels are safe for most people have side effects that are infrequent, but potentially serious such as liver and muscle problems. Ezetimibe - lowers LDL (bad) cholesterol   - may be used with statins or alone  -  acts within the intestine to block absorption of cholesterol. lowers LDL (bad) cholesterol may be used with statins or alone acts within the intestine to block absorption of cholesterol. Bile acid sequestrants - lower LDL (bad) cholesterol levels  - are sometimes prescribed with statins  - are not usually prescribed alone to lower cholesterol. lower LDL (bad) cholesterol levels are sometimes prescribed with statins are not usually prescribed alone to lower cholesterol. Nicotinic acid - lowers LDL (bad) cholesterol and triglycerides, and raises HDL (good) cholesterol  - should be used only under a doctor's supervision. lowers LDL (bad) cholesterol and triglycerides, and raises HDL (good) cholesterol should be used only under a doctor's supervision. Fibrates - mainly lower triglycerides  - may increase HDL (good) cholesterol levels  - may increase the risk of muscle problems when used with a statin. mainly lower triglycerides may increase HDL (good) cholesterol levels may increase the risk of muscle problems when used with a statin.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " High blood cholesterol, also known as hypercholesterolemia, is a condition characterized by elevated levels of cholesterol in the blood. Cholesterol is a waxy, fat-like substance that is essential for building cells, but high levels can lead to health problems.\n",
      "\n",
      "There are two main types of cholesterol:\n",
      "\n",
      "1. Low-Density Lipoprotein (LDL): Often referred to as \"bad\" cholesterol because high levels can lead to plaque buildup in arteries, increasing the risk of heart disease and stroke.\n",
      "2. High-Density Lipoprotein (HDL): Known as \"good\" cholesterol because it helps remove other forms of cholesterol from the bloodstream.\n",
      "\n",
      "High blood cholesterol is typically diagnosed through a blood test called a lipid panel, which measures the levels of total cholesterol, LDL, HDL, and triglycerides.\n",
      "\n",
      "Risk factors for high blood cholesterol include:\n",
      "\n",
      "- Unhealthy diet (high in saturated fats and trans fats)\n",
      "- Lack of physical activity\n",
      "- Obesity\n",
      "- Smoking\n",
      "- Diabetes\n",
      "- Family history of high cholesterol\n",
      "\n",
      "Managing high blood cholesterol often involves lifestyle changes such as:\n",
      "\n",
      "- Eating a heart-healthy diet rich in fruits, vegetables, whole grains, and lean proteins\n",
      "- Regular physical activity\n",
      "- Maintaining a healthy weight\n",
      "- Avoiding tobacco smoke\n",
      "- Limiting alcohol consumption\n",
      "\n",
      "In some cases, medications such as statins may be prescribed to help lower cholesterol levels.\n",
      "\n",
      "It's important to consult with a healthcare provider for personalized advice and treatment options.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧪 TEST 9\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What are the symptoms of Multiple endocrine neoplasia type 2A ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "What are the signs and symptoms of Multiple endocrine neoplasia type 2A? The Human Phenotype Ontology provides the following list of signs and symptoms for Multiple endocrine neoplasia type 2A. If the information is available, the table below includes how often the symptom is seen in people with this condition. You can use the MedlinePlus Medical Dictionary to look up the definitions for these medical terms. Signs and Symptoms Approximate number of patients (when available) Abnormality of the integument - Aganglionic megacolon - Autosomal dominant inheritance - Elevated calcitonin - Elevated urinary epinephrine - Hypercortisolism - Hyperparathyroidism - Hypertension - Medullary thyroid carcinoma - Parathyroid adenoma - Pheochromocytoma - The Human Phenotype Ontology (HPO) has collected information on how often a sign or symptom occurs in a condition. Much of this information comes from Orphanet, a European rare disease database. The frequency of a sign or symptom is usually listed as a rough estimate of the percentage of patients who have that feature. The frequency may also be listed as a fraction. The first number of the fraction is how many people had the symptom, and the second number is the total number of people who were examined in one study. For example, a frequency of 25/25 means that in a study of 25 people all patients were found to have that symptom. Because these frequencies are based on a specific study, the fractions may be different if another group of patients are examined. Sometimes, no information on frequency is available. In these cases, the sign or symptom may be rare or common.\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " To answer this question, I will follow these steps:\n",
      "\n",
      "1. **Identify the key terms**: Multiple endocrine neoplasia type 2A (MEN2A) is the key term here.\n",
      "2. **Research the symptoms**: MEN2A is a genetic disorder that affects the endocrine glands, leading to tumors in these glands.\n",
      "3. **List the symptoms**: The symptoms of MEN2A include medullary thyroid carcinoma, pheochromocytoma, and hyperparathyroidism.\n",
      "4. **Provide a detailed explanation**: Medullary thyroid carcinoma is a cancer of the thyroid gland, pheochromocytoma is a tumor of the adrenal gland that can cause high blood pressure, and hyperparathyroidism is an overactivity of the parathyroid glands leading to high calcium levels in the blood.\n",
      "\n",
      "The symptoms of Multiple endocrine neoplasia type 2A include medullary thyroid carcinoma, pheochromocytoma, and hyperparathyroidism.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "================================================================================\n",
      "🧪 TEST 10\n",
      "================================================================================\n",
      "\n",
      "🤔 QUESTION:\n",
      "----------------------------------------\n",
      "What is (are) Dry Eye ?\n",
      "\n",
      "✅ ORIGINAL ANSWER:\n",
      "----------------------------------------\n",
      "National Eye Institute  National Institutes of Health  2020 Vision Place  Bethesda, MD 20892-3655  301-496-5248  E-mail: 2020@nei.nih.gov  www.nei.nih.gov\n",
      "\n",
      "🤖 FINE-TUNED AI RESPONSE:\n",
      "----------------------------------------\n",
      " Dry eye is a condition where there is insufficient lubrication on the surface of the eye, leading to discomfort, irritation, and potential damage to the eye. It can be caused by various factors, including environmental conditions, aging, certain medications, or underlying health issues. Symptoms often include a gritty or burning sensation, redness, blurred vision, and increased sensitivity to light. Treatment typically involves artificial tears, lifestyle changes, and in some cases, medications or surgery to address the underlying cause.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_test_result(iterations = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0fdb8021-ed46-4077-9306-517c054432de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd28d75e-4a25-4f1a-9b35-b394bf24c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_medical_evaluation(model_path, adapter_path=\"./lora_adapter\", tasks=None):\n",
    "   \n",
    "    results = {}\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs(\"./evaluations\", exist_ok=True)\n",
    "    \n",
    "    for task in medical_tasks:\n",
    "        print(f\"Evaluating {task}...\")\n",
    "        \n",
    "        # Build model args with adapter\n",
    "        model_args = f\"pretrained={model_path},peft={adapter_path}\"\n",
    "        \n",
    "        cmd = [\n",
    "            \"lm_eval\",\n",
    "            \"--model\", \"hf\",\n",
    "            \"--model_args\", model_args,\n",
    "            \"--tasks\", task,\n",
    "            \"--batch_size\", \"4\",  # Reduced for stability\n",
    "            \"--output_path\", f\"./evaluations/{task}\"\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                # Try to load results\n",
    "                result_file = f\"./evaluations/{task}/evaluations.json\"\n",
    "                if os.path.exists(result_file):\n",
    "                    with open(result_file) as f:\n",
    "                        task_results = json.load(f)\n",
    "                    results[task] = task_results\n",
    "                    print(f\"✅ {task} completed\")\n",
    "                else:\n",
    "                    print(f\"❌ {task} - results file not found\")\n",
    "            else:\n",
    "                print(f\"❌ {task} failed: {result.stderr}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error evaluating {task}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8877833-3457-48f8-a506-487f05f1a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating medqa...\n",
      "❌ medqa failed: 2025-08-17:21:00:33 ERROR    [__main__:419] Tasks were not found: medqa\n",
      "                                               Try `lm-eval --tasks list` for list of available tasks\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 423, in cli_evaluate\n",
      "    raise ValueError(\n",
      "ValueError: Tasks not found: medqa. Try `lm-eval --tasks {list_groups,list_subtasks,list_tags,list}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above, or pass '--verbosity DEBUG' to troubleshoot task registration issues.\n",
      "\n",
      "Evaluating medmcqa...\n",
      "❌ medmcqa failed: 2025-08-17:21:00:44 INFO     [__main__:446] Selected Tasks: ['medmcqa']\n",
      "2025-08-17:21:00:44 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:00:44 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:00:44 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating pubmedqa...\n",
      "❌ pubmedqa failed: 2025-08-17:21:00:55 INFO     [__main__:446] Selected Tasks: ['pubmedqa']\n",
      "2025-08-17:21:00:55 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:00:55 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:00:55 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_anatomy...\n",
      "❌ mmlu_anatomy failed: 2025-08-17:21:01:05 INFO     [__main__:446] Selected Tasks: ['mmlu_anatomy']\n",
      "2025-08-17:21:01:05 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:01:05 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:01:05 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_clinical_knowledge...\n",
      "❌ mmlu_clinical_knowledge failed: 2025-08-17:21:01:15 INFO     [__main__:446] Selected Tasks: ['mmlu_clinical_knowledge']\n",
      "2025-08-17:21:01:15 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:01:15 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:01:15 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_college_medicine...\n",
      "❌ mmlu_college_medicine failed: 2025-08-17:21:01:25 INFO     [__main__:446] Selected Tasks: ['mmlu_college_medicine']\n",
      "2025-08-17:21:01:25 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:01:25 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:01:25 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_medical_genetics...\n",
      "❌ mmlu_medical_genetics failed: 2025-08-17:21:01:35 INFO     [__main__:446] Selected Tasks: ['mmlu_medical_genetics']\n",
      "2025-08-17:21:01:35 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:01:35 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:01:35 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_professional_medicine...\n",
      "❌ mmlu_professional_medicine failed: 2025-08-17:21:01:45 INFO     [__main__:446] Selected Tasks: ['mmlu_professional_medicine']\n",
      "2025-08-17:21:01:45 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:01:45 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': './lora_adapter'}\n",
      "2025-08-17:21:01:46 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating medqa...\n",
      "❌ medqa failed: 2025-08-17:21:01:56 ERROR    [__main__:419] Tasks were not found: medqa\n",
      "                                               Try `lm-eval --tasks list` for list of available tasks\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 423, in cli_evaluate\n",
      "    raise ValueError(\n",
      "ValueError: Tasks not found: medqa. Try `lm-eval --tasks {list_groups,list_subtasks,list_tags,list}` to list out all available names for task groupings; only (sub)tasks; tags; or all of the above, or pass '--verbosity DEBUG' to troubleshoot task registration issues.\n",
      "\n",
      "Evaluating medmcqa...\n",
      "❌ medmcqa failed: 2025-08-17:21:02:06 INFO     [__main__:446] Selected Tasks: ['medmcqa']\n",
      "2025-08-17:21:02:06 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:02:06 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:02:07 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating pubmedqa...\n",
      "❌ pubmedqa failed: 2025-08-17:21:02:17 INFO     [__main__:446] Selected Tasks: ['pubmedqa']\n",
      "2025-08-17:21:02:17 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:02:17 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:02:17 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_anatomy...\n",
      "❌ mmlu_anatomy failed: 2025-08-17:21:02:28 INFO     [__main__:446] Selected Tasks: ['mmlu_anatomy']\n",
      "2025-08-17:21:02:28 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:02:28 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:02:28 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_clinical_knowledge...\n",
      "❌ mmlu_clinical_knowledge failed: 2025-08-17:21:02:38 INFO     [__main__:446] Selected Tasks: ['mmlu_clinical_knowledge']\n",
      "2025-08-17:21:02:38 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:02:38 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:02:38 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_college_medicine...\n",
      "❌ mmlu_college_medicine failed: 2025-08-17:21:02:48 INFO     [__main__:446] Selected Tasks: ['mmlu_college_medicine']\n",
      "2025-08-17:21:02:48 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:02:48 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:02:48 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_medical_genetics...\n",
      "❌ mmlu_medical_genetics failed: 2025-08-17:21:02:59 INFO     [__main__:446] Selected Tasks: ['mmlu_medical_genetics']\n",
      "2025-08-17:21:02:59 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:02:59 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:02:59 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "Evaluating mmlu_professional_medicine...\n",
      "❌ mmlu_professional_medicine failed: 2025-08-17:21:03:09 INFO     [__main__:446] Selected Tasks: ['mmlu_professional_medicine']\n",
      "2025-08-17:21:03:09 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-08-17:21:03:09 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': './lora_adapter', 'peft': \"['medqa'\", \" 'medmcqa'\": '', \" 'pubmedqa'\": '', \"\n",
      "        'mmlu_anatomy'\": '', \" 'mmlu_clinical_knowledge'\": '', \" 'mmlu_college_medicine'\": '', \" 'mmlu_medical_genetics'\": '', \"\n",
      "        'mmlu_professional_medicine']\": ''}\n",
      "2025-08-17:21:03:09 INFO     [models.huggingface:147] Using device 'cuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 176, in __init__\n",
      "    self._get_config(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lm_eval/models/huggingface.py\", line 558, in _get_config\n",
      "    self._config = transformers.AutoConfig.from_pretrained(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\", line 1291, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized model in ./lora_adapter. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, doge, donut-swin, dots1, dpr, dpt, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, minimax, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "model_path = \"./lora_adapter\"\n",
    "results = run_medical_evaluation(model_path)\n",
    "medical_tasks = [\n",
    "    \"medqa\",\n",
    "    \"medmcqa\", \n",
    "    \"pubmedqa\",\n",
    "    \"mmlu_anatomy\",\n",
    "    \"mmlu_clinical_knowledge\",\n",
    "    \"mmlu_college_medicine\",\n",
    "    \"mmlu_medical_genetics\",\n",
    "    \"mmlu_professional_medicine\"\n",
    "]\n",
    "medical_results = run_medical_evaluation(model_path, medical_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10e88f-a94f-486e-8e86-c34003fed10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
