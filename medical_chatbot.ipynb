{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df07268-c9ef-4b37-b45d-d4d601966c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Hugging Face libraries\n",
    "# %pip install  --upgrade \\\n",
    "#   \"tensorboard\" \\\n",
    "#   \"flash-attn\" \\\n",
    "#   \"liger-kernel\" \\\n",
    "#   \"setuptools\" \\\n",
    "#   \"deepspeed\" \\\n",
    "#   \"lm-eval[api]\" \\\n",
    "#   \"torch\"\\\n",
    "#   \"torchvision\" \\\n",
    "#   \"transformers\" \\\n",
    "#   \"datasets\" \\\n",
    "#   \"accelerate\" \\\n",
    "#   \"bitsandbytes\" \\\n",
    "#   \"trl\" \\\n",
    "#   \"peft\" \\\n",
    "#   \"lighteval\" \\\n",
    "#   \"hf-transfer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022cb1e-ffe6-4d5b-bfee-1d2b5d1339b9",
   "metadata": {},
   "source": [
    "### IMport libraries and framewoerks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c51ba00-cba9-4157-a7f5-3b9fd9191a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import is_liger_kernel_available\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, TrlParser, ModelConfig, SFTConfig, get_peft_config\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c19ae2-45c8-4a2c-80af-baf75d28704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a753c-87ce-48c7-965e-d7a3a8f5622a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d5aeb8-8244-4f0f-a7ff-3574ccc536bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ba1353-741d-4512-a3e9-f369f2a20624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mle_screening_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8111983e-0824-418c-ba09-1aafbef4daaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16406, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495f030d-b752-49bc-a947-5e3a9a17ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is (are) Guillain-Barre syndrome ?\n",
      "\n",
      "Answer: Guillain-Barr syndrome is a rare disorder in which the body's immune system attacks part of the peripheral nervous system. Symptoms include muscle weakness, numbness, and tingling sensations, which can increase in intensity until the muscles cannot be used at all. Usually Guillain-Barr syndrome occurs a few days or weeks after symptoms of a viral infection. Occasionally, surgery or vaccinations will trigger the syndrome. It remains unclear why only some people develop Guillain-Barr syndrome but there may be a genetic predisposition in some cases. Diagnosed patients should be admitted to a hospital for early treatment. There is no cure for Guillain-Barr syndrome, but treatments such as plasma exchange (plasmapheresis) and high dose immunoglobulins may reduce the severity and duration of symptoms. Recovery can take as little as a few days to as long as a few years. About 30% of those with Guillain-Barr syndrome have residual weakness. A small number may suffer a relapse many years after the initial attack.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "sample = df.sample()\n",
    "print(f\"Question: {sample['question'].values[0]}\\n\")\n",
    "print(f\"Answer: {sample['answer'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba1aa8-01ca-4ac3-ac82-79ce01198d47",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52799497-1ac6-40a2-be58-2fb72b3beccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7f6bdd4e684d1eab50ab15ef584419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/MediPhi-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b6344-c617-49f4-8f50-efeef94395f7",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd077c65-4a4a-4afa-9b15-b4a20bece82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from pandas\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# First split: 80% train, 20% temp (val + test)\n",
    "train_temp_split = dataset.train_test_split(test_size=0.25, seed=42)\n",
    "train_dataset = train_temp_split[\"train\"]  # 80% of data\n",
    "temp_dataset = train_temp_split[\"test\"]    # 20% of data\n",
    "\n",
    "# Second split: Split temp into 12.5% validation, 12.5% test\n",
    "val_test_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "val_dataset = val_test_split[\"train\"]      # 12.5% of original data\n",
    "test_dataset = val_test_split[\"test\"]      # 12.5% of original data\n",
    "\n",
    "# Step 3: Create a DatasetDict to store all splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc2c430-0258-4d77-abd6-cbeff0fb702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 12304\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afa829f6-c8ce-4b27-a00f-5e393d8cb313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47c91694444903948f080276a57051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc41ea479dc491dbb8184b6cd266d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96929c87bdd4206ac9efa53f3c5a0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Create system prompt\n",
    "system_message = \"\"\"\n",
    "You are a smart medical assiatnt to help user question about their queries\n",
    "\n",
    "To answer question, follow the following instructions:\n",
    "1. **Understand the question**: Clearly identify the question and any important given values.\n",
    "3. **Answer Step-by-Step**: Iteratively progress your answer\n",
    "4. **Double Check**: If applicable, double check the question for accuracy and sense.\n",
    "\"\"\"\n",
    " \n",
    "# Remove the existing \"text\" column if it exists to avoid conflicts\n",
    "def processes_data(sample):\n",
    "    question = str(sample[\"question\"] or \"\").strip()\n",
    "    answer = str(sample[\"answer\"] or \"\").strip()\n",
    "    \n",
    "    if not question or not answer:\n",
    "        return {\"text\": \"\"}  # Always return string\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}  # Always return string\n",
    "\n",
    "# Remove existing text column and apply preprocessing\n",
    "dataset = dataset.remove_columns([\"text\"] if \"text\" in dataset['train'].column_names else [])\n",
    "dataset = dataset.map(processes_data, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfaebe5-82b8-41da-812d-afdb5a42d608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 12304\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e27f99d-033b-493a-bb7a-c26d47cc00db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many people are affected by Denys-Drash syndrome ?',\n",
       " 'answer': 'The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.',\n",
       " 'text': '<|system|>\\n\\nYou are a smart medical assiatnt to help user question about their queries\\n\\nTo answer question, follow the following instructions:\\n1. **Understand the question**: Clearly identify the question and any important given values.\\n3. **Answer Step-by-Step**: Iteratively progress your answer\\n4. **Double Check**: If applicable, double check the question for accuracy and sense.\\n<|end|>\\n<|user|>\\nHow many people are affected by Denys-Drash syndrome ?<|end|>\\n<|assistant|>\\nThe prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.<|end|>\\n<|endoftext|>'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7e74d-69e1-4025-a72e-cbd2e158521b",
   "metadata": {},
   "source": [
    "### Understand model architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1c11530-6238-4985-9920-c4a068c06e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d669f8c2-fe83-473a-b921-6b33a38110b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3821079552 || all params: 3821079552 || trainable%: 100.00%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(\n",
    "    f\"trainable params: {trainable_params} || \"\n",
    "    f\"all params: {all_param} || \"\n",
    "    f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26bb7079-2cae-4592-aca8-058e92c788eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='microsoft/MediPhi-Instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77585eda-865a-40df-b8fe-dbb2902cbb81",
   "metadata": {},
   "source": [
    "### Before training test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9632c1d1-684f-4b6e-aed7-62a4e21252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, StoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60c0bb10-01ba-4472-a77a-eb4f65579d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.\n",
      "\n",
      "Answer: The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.\n"
     ]
    }
   ],
   "source": [
    "question = dataset['test'][0]['answer']\n",
    "answer = dataset['test'][0]['answer']\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1b8145c-069f-48cb-849d-d0cfcc3104a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 32007: '<|end|>'\n"
     ]
    }
   ],
   "source": [
    "# Check what token ID 32007 represents\n",
    "print(f\"Token 32007: '{tokenizer.decode([32007])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62375c13-c37d-4fac-80cf-b15eac101e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Denys-Drash syndrome is a rare genetic disorder, and its exact prevalence is not well-documented. However, at least 150 cases have been reported in scientific literature, indicating that it is a rare condition.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/MediPhi-Instruct\n",
    "prompt = \"Operative Report:\\nPerformed: Cholecystectomy\\nOperative Findings: The gallbladder contained multiple stones and had thickening of its wall. Mild peritoneal fluid was noted.\"\n",
    "\n",
    "# Hugging Face pipeline for text generation does apply apply_chat_template under the hood. \n",
    "# So we do not need to process for the text generation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "#  stops generation when the model generates token ID 32007\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "  def __init__(self, eos_sequence = [32007]):\n",
    "      self.eos_sequence = eos_sequence\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "      last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "      return self.eos_sequence in last_ids\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "    \"stopping_criteria\": [EosListStoppingCriteria()]\n",
    "\n",
    "}\n",
    "output = pipe(messages, **generation_args)\n",
    "print(f\"AI: {output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec004dc0-c044-4160-b18e-5c2574522134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad53fb5f-c6b7-4300-980c-9383462cf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above testing, it is clear that Medphi is generating more or less similar text generation.\n",
    "# WIth fine tiuning the model might learn more numances of the dataset provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44810f-cbd2-460c-a656-eb3c4cb20bd2",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17809db4-7fe1-4198-9487-aa7bf214f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_lorafa_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93fc29b6-b047-427e-9e1d-d9e798281b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff368b32-d4ab-48b8-a1ef-d455a4aeb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "max_seq_length = 2048\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b156dea7-7b0b-4d93-8ce8-e5a632af68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    bias=\"none\",\n",
    "    target_modules = ['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17e74f48-1be7-4d50-834f-ebce68cf2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)\n",
    "optimizer = create_lorafa_optimizer(\n",
    "    model=peft_model,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lr=7e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b41c979-04b5-4e00-bacc-952b68d6096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,252,928 || all params: 3,846,245,376 || trainable%: 0.4226\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9c81eb-7bd7-4191-8072-70e55f16c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/medphi-chat-v0'\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    # Basic training parameters\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",  # Memory efficient optimizer\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",  # or \"wandb\" if you use weights & biases\n",
    "    \n",
    "    # Memory and performance\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Mixed precision training\n",
    "    fp16=False,  # Set to True if your GPU supports it\n",
    "    bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "    \n",
    "    # SFT-specific parameters\n",
    "    max_length=2048,\n",
    "    packing=True,  # Pack multiple short sequences into one\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=0.3,\n",
    "    gradient_checkpointing=True,  # Save memory at cost of speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68b3465c-c232-41d0-89cf-0863144bf03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'microsoft/MediPhi-Instruct' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a304bd7cbfdf4aed8e7da44a7e6319b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ef3d8936554d0f82c3e58318036629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d293bf0525945d7837dd6f643f9ef94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e035affc3ed4da3a19b18d16c79fea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7a18cf18764885a2f3867bbf8f31eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611bb696f4474d46bb4f7cc195073c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-17 17:27:47,568] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: No such file or directory\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-17 17:27:48,989] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['val'],\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba84bc5-2a46-416f-82c1-1690d76d1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 57/480 06:52 < 52:55, 0.13 it/s, Epoch 0.35/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e749e-3cfb-43c9-b6b4-3d16437f84e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
