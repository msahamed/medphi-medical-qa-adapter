{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df07268-c9ef-4b37-b45d-d4d601966c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # Install Hugging Face libraries\n",
    "# %pip install  --upgrade \\\n",
    "#   \"evaluate\" \\\n",
    "#   \"tensorboard\" \\\n",
    "#   \"flash-attn\" \\\n",
    "#   \"liger-kernel\" \\\n",
    "#   \"setuptools\" \\\n",
    "#   \"deepspeed\" \\\n",
    "#   \"lm-eval[api]\" \\\n",
    "#   \"torch\"\\\n",
    "#   \"torchvision\" \\\n",
    "#   \"transformers\" \\\n",
    "#   \"datasets\" \\\n",
    "#   \"accelerate\" \\\n",
    "#   \"bitsandbytes\" \\\n",
    "#   \"trl\" \\\n",
    "#   \"peft\" \\\n",
    "#   \"lighteval\" \\\n",
    "#   \"hf-transfer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022cb1e-ffe6-4d5b-bfee-1d2b5d1339b9",
   "metadata": {},
   "source": [
    "### Import libraries and frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c51ba00-cba9-4157-a7f5-3b9fd9191a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import is_liger_kernel_available\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, TrlParser, ModelConfig, SFTConfig, get_peft_config\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cb671",
   "metadata": {},
   "source": [
    "### Check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c19ae2-45c8-4a2c-80af-baf75d28704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a753c-87ce-48c7-965e-d7a3a8f5622a",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ba1353-741d-4512-a3e9-f369f2a20624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mle_screening_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8111983e-0824-418c-ba09-1aafbef4daaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16406, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495f030d-b752-49bc-a947-5e3a9a17ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the genetic changes related to Tourette syndrome ?\n",
      "\n",
      "Answer: A variety of genetic and environmental factors likely play a role in causing Tourette syndrome. Most of these factors are unknown, and researchers are studying risk factors before and after birth that may contribute to this complex disorder. Scientists believe that tics may result from changes in brain chemicals (neurotransmitters) that are responsible for producing and controlling voluntary movements.  Mutations involving the SLITRK1 gene have been identified in a small number of people with Tourette syndrome. This gene provides instructions for making a protein that is active in the brain. The SLITRK1 protein probably plays a role in the development of nerve cells, including the growth of specialized extensions (axons and dendrites) that allow each nerve cell to communicate with nearby cells. It is unclear how mutations in the SLITRK1 gene can lead to this disorder.  Most people with Tourette syndrome do not have a mutation in the SLITRK1 gene. Because mutations have been reported in so few people with this condition, the association of the SLITRK1 gene with this disorder has not been confirmed. Researchers suspect that changes in other genes, which have not been identified, are also associated with Tourette syndrome.\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "sample = df.sample()\n",
    "print(f\"Question: {sample['question'].values[0]}\\n\")\n",
    "print(f\"Answer: {sample['answer'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba1aa8-01ca-4ac3-ac82-79ce01198d47",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1086e7-bcc2-49f4-8af3-46d1e964ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BitsAndBytesConfig for quantization that helps to reduce model size\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c6b4c-de32-48cf-8bf1-5470fb7f735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23f1e9c2b5a40049180ad3232f6ee46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. APPLY QUANTIZATION (This was missing!)\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 for better memory efficiency\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Load model WITH quantization\n",
    "model_name = \"microsoft/MediPhi-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,  \n",
    "    torch_dtype=torch.bfloat16,      # Use bfloat16 for memory efficiency\n",
    "    device_map=\"auto\",               # Automatically distribute across GPUs\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"  # Ensure consistent padding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52799497-1ac6-40a2-be58-2fb72b3beccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a6c67694e342a7b1ce5a7b9de929b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/MediPhi-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b6344-c617-49f4-8f50-efeef94395f7",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd077c65-4a4a-4afa-9b15-b4a20bece82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from pandas\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# First split: 75% train, 25% temp (val + test)\n",
    "train_temp_split = dataset.train_test_split(test_size=0.25, seed=42)\n",
    "train_dataset = train_temp_split[\"train\"]  # 80% of data\n",
    "temp_dataset = train_temp_split[\"test\"]    # 20% of data\n",
    "\n",
    "# Second split: Split temp into 12.5% validation, 12.5% test\n",
    "val_test_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "val_dataset = val_test_split[\"train\"]      # 12.5% of original data\n",
    "test_dataset = val_test_split[\"test\"]      # 12.5% of original data\n",
    "\n",
    "# Step 3: Create a DatasetDict to store all splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc2c430-0258-4d77-abd6-cbeff0fb702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 12304\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afa829f6-c8ce-4b27-a00f-5e393d8cb313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df1fe6ce571498e8429e06ae3050380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173758d5090047a6935acdcca92beb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3479fb92f44611b18ac6120f6d7baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Create system prompt\n",
    "system_message = \"\"\"\n",
    "You are a smart medical assiatnt to help user question about their queries\n",
    "\n",
    "To answer question, follow the following instructions:\n",
    "1. **Understand the question**: Clearly identify the question and any important given values.\n",
    "3. **Answer Step-by-Step**: Iteratively progress your answer\n",
    "4. **Double Check**: If applicable, double check the question for accuracy and sense.\n",
    "\"\"\"\n",
    " \n",
    "# Remove the existing \"text\" column if it exists to avoid conflicts\n",
    "def processes_data(sample):\n",
    "    question = str(sample[\"question\"] or \"\").strip()\n",
    "    answer = str(sample[\"answer\"] or \"\").strip()\n",
    "    \n",
    "    if not question or not answer:\n",
    "        return {\"text\": \"\"}  # Always return string\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    return {\"text\": text}  # Always return string\n",
    "\n",
    "# Remove existing text column and apply preprocessing\n",
    "dataset = dataset.remove_columns([\"text\"] if \"text\" in dataset['train'].column_names else [])\n",
    "dataset = dataset.map(processes_data, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bfaebe5-82b8-41da-812d-afdb5a42d608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 12304\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'text'],\n",
       "        num_rows: 2051\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e27f99d-033b-493a-bb7a-c26d47cc00db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many people are affected by Denys-Drash syndrome ?',\n",
       " 'answer': 'The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.',\n",
       " 'text': '<|system|>\\n\\nYou are a smart medical assiatnt to help user question about their queries\\n\\nTo answer question, follow the following instructions:\\n1. **Understand the question**: Clearly identify the question and any important given values.\\n3. **Answer Step-by-Step**: Iteratively progress your answer\\n4. **Double Check**: If applicable, double check the question for accuracy and sense.\\n<|end|>\\n<|user|>\\nHow many people are affected by Denys-Drash syndrome ?<|end|>\\n<|assistant|>\\nThe prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.<|end|>\\n<|endoftext|>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b436c4a9-0e65-4124-a342-a2d07943ddc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd823cf25e534afbae8fef5adc9a6e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df55c7f3b53549d6a5c7c9957aa2cf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48fd14e37424c4b8c8054571531a578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"./dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7e74d-69e1-4025-a72e-cbd2e158521b",
   "metadata": {},
   "source": [
    "### Understand model architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1c11530-6238-4985-9920-c4a068c06e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d669f8c2-fe83-473a-b921-6b33a38110b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3821079552 || all params: 3821079552 || trainable%: 100.00%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(\n",
    "    f\"trainable params: {trainable_params} || \"\n",
    "    f\"all params: {all_param} || \"\n",
    "    f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26bb7079-2cae-4592-aca8-058e92c788eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='microsoft/MediPhi-Instruct', vocab_size=32000, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77585eda-865a-40df-b8fe-dbb2902cbb81",
   "metadata": {},
   "source": [
    "### Before training test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9632c1d1-684f-4b6e-aed7-62a4e21252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, StoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c0bb10-01ba-4472-a77a-eb4f65579d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.\n",
      "\n",
      "Answer: The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.\n"
     ]
    }
   ],
   "source": [
    "question = dataset['test'][0]['answer']\n",
    "answer = dataset['test'][0]['answer']\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1b8145c-069f-48cb-849d-d0cfcc3104a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 32007: '<|end|>'\n"
     ]
    }
   ],
   "source": [
    "# Check what token ID 32007 represents\n",
    "print(f\"Token 32007: '{tokenizer.decode([32007])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62375c13-c37d-4fac-80cf-b15eac101e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Denys-Drash syndrome is a rare genetic disorder, and its exact prevalence is not well-documented. However, at least 150 cases have been reported in scientific literature, indicating that it is a rare condition.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/MediPhi-Instruct\n",
    "prompt = \"Operative Report:\\nPerformed: Cholecystectomy\\nOperative Findings: The gallbladder contained multiple stones and had thickening of its wall. Mild peritoneal fluid was noted.\"\n",
    "\n",
    "# Hugging Face pipeline for text generation does apply apply_chat_template under the hood. \n",
    "# So we do not need to process for the text generation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "#  stops generation when the model generates token ID 32007\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "  def __init__(self, eos_sequence = [32007]):\n",
    "      self.eos_sequence = eos_sequence\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "      last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "      return self.eos_sequence in last_ids\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "    \"stopping_criteria\": [EosListStoppingCriteria()]\n",
    "\n",
    "}\n",
    "output = pipe(messages, **generation_args)\n",
    "print(f\"AI: {output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec004dc0-c044-4160-b18e-5c2574522134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The prevalence of Denys-Drash syndrome is unknown; at least 150 affected individuals have been reported in the scientific literature.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad53fb5f-c6b7-4300-980c-9383462cf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above testing, it is clear that Medphi is generating more or less similar text generation.\n",
    "# WIth fine tiuning the model might learn more numances of the dataset provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44810f-cbd2-460c-a656-eb3c4cb20bd2",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17809db4-7fe1-4198-9487-aa7bf214f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_lorafa_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b156dea7-7b0b-4d93-8ce8-e5a632af68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    bias=\"none\",\n",
    "    target_modules = ['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17e74f48-1be7-4d50-834f-ebce68cf2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b41c979-04b5-4e00-bacc-952b68d6096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a03028a-85f9-4b0f-9c19-5093986f5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Custom metrics for medical/chat evaluation\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. BLEU Score (text similarity)\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_score = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    metrics.update(bleu_score)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c81eb-7bd7-4191-8072-70e55f16c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os \n",
    "\n",
    "# Set environment variables to fix tokenizer warnings and memory issues\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    # Basic training parameters\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    save_total_limit=2,\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # Memory and performance\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Mixed precision training\n",
    "    bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "    \n",
    "    # SFT-specific parameters\n",
    "    max_length=1024,\n",
    "    packing=True,  # Pack multiple short sequences into one\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=0.3,\n",
    "    gradient_checkpointing=True,  # Save memory at cost of speed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68b3465c-c232-41d0-89cf-0863144bf03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'microsoft/MediPhi-Instruct' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:453: UserWarning: Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:495: UserWarning: You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f6b06cb1ae494ea2d20a95303b6a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811ac19a61fe4b23a787d05aebc67355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfd0606f9884dd39771b827e6f97479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing train dataset:   0%|          | 0/12304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2af54fc1a74c11ae98eac806bfa079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804f2bce7e954d698d1a741b265bae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04db1a45a534dbcaae80cac14a4b7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Packing eval dataset:   0%|          | 0/2051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-17 21:15:18,836] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-17 21:15:20,461] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['val'],\n",
    "    peft_config=lora_config,\n",
    "    # compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba84bc5-2a46-416f-82c1-1690d76d1b6f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Step | Training Loss | Validation Loss |\n",
    "# |------|---------------|-----------------|\n",
    "# | 20   | 1.179900      | 0.830046        |\n",
    "# | 40   | 0.783400      | 0.716754        |\n",
    "# | 60   | 0.718500      | 0.689175        |\n",
    "# | 80   | 0.683800      | 0.670964        |\n",
    "# | 100  | 0.664100      | 0.659222        |\n",
    "# | 120  | 0.663800      | 0.651528        |\n",
    "# | 140  | 0.644100      | 0.644649        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7c550-d96c-4e8a-90b0-2e9603eedd63",
   "metadata": {},
   "source": [
    "### Save teh adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb5700-870c-424c-b978-aa5ba5865dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora_adapter(trainer, save_path=\"./lora_adapter\"):\n",
    "    \n",
    "    # Save the adapter\n",
    "    trainer.model.save_pretrained(save_path)\n",
    "    trainer.tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    print(f\"LoRA adapter saved to: {save_path}\")\n",
    "    print(f\"Adapter size: {get_directory_size(save_path):.2f} MB\")\n",
    "    \n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c843e0-a69a-4b32-a977-b1ef9116360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09838978-5588-4619-82bf-3a90f4038235",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = save_lora_adapter(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0d103-41a1-467c-9888-a64944d18e60",
   "metadata": {},
   "source": [
    "## Push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3598eb-9e10-47e7-ad7e-2e38a9881d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5687c-634f-47e0-a023-d3dc82850862",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'removed it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b8423-1d2d-41bf-8b19-ba8dbfeb2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4b54b-8789-4901-9e1d-360067a823b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"sabber/medphi-medical-qa-adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14509f2a-f214-4b24-be1c-06876625cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_repo(repo_id=repo_id, token=token, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80cb88-654e-4d9b-af61-9d24d2c158d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_upload = [\n",
    "    \"adapter_config.json\",\n",
    "    \"adapter_model.safetensors\",  # or adapter_model.bin\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"special_tokens_map.json\"\n",
    "]\n",
    "\n",
    "for file in files_to_upload:\n",
    "    file_path = os.path.join(adapter_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=file,\n",
    "            repo_id=repo_id,\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"Uploaded: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d756ebf-a972-4296-8b10-ef7d76571eef",
   "metadata": {},
   "source": [
    "### Test model with trained adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a21a1-78d3-4b7b-b053-e668d1b942fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fdce60-7576-40c9-8cde-451b704423bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e31aab-b6ef-46ce-b791-380a223cbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a55581-b7a6-4d99-b4cf-a1e6224b0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, let's check the current adapter config\n",
    "with open(\"./lora_adapter/adapter_config.json\", \"r\") as f:\n",
    "    adapter_config = json.load(f)\n",
    "\n",
    "print(\"Current adapter config:\")\n",
    "print(adapter_config)\n",
    "\n",
    "# 2. Add the missing base model path if it's not there\n",
    "if \"base_model_name_or_path\" not in adapter_config or adapter_config[\"base_model_name_or_path\"] is None:\n",
    "    adapter_config[\"base_model_name_or_path\"] = \"microsoft/MediPhi-Instruct\"\n",
    "    \n",
    "    # Save the fixed config\n",
    "    with open(\"./lora_adapter/adapter_config.json\", \"w\") as f:\n",
    "        json.dump(adapter_config, f, indent=2)\n",
    "    \n",
    "    print(\"✅ Fixed adapter_config.json with base model path\")\n",
    "else:\n",
    "    print(\"✅ Base model path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc89cd1-c207-412b-b85f-0f758c1ab25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"./lora_adapter\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c261c2-91bb-46ee-86a4-c7d3354b634b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
